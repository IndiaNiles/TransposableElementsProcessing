{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Process all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T21:17:25.506115900Z",
     "start_time": "2023-12-17T21:17:25.496101900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, os\n",
    "import pandas as pd\n",
    "\n",
    "def process_data(filename, data):\n",
    "    # create stats data\n",
    "    ltr_gypsy_stats = {}\n",
    "    ltr_copia_stats = {}\n",
    "\n",
    "    # add genome column\n",
    "    genome = re.findall(\"^[a-zA-Z]*\", filename)[0]\n",
    "    data['Genome'] = genome\n",
    "    ltr_gypsy_stats['Genome'] = genome\n",
    "    ltr_copia_stats['Genome'] = genome\n",
    "\n",
    "    # filter for intact elements\n",
    "    if filename.find(\"intact\") == -1:\n",
    "        data = data[data['#TE'].str.find(\"intact\") != -1]\n",
    "    ltr_gypsy_stats['Intact_Elements'] = len(data.index)\n",
    "    ltr_copia_stats['Intact_Elements'] = len(data.index)\n",
    "\n",
    "    # EDTA/TEsorter classification count\n",
    "    ltr_gypsy = data[(data['#TE'].str.find('LTR') != -1) & (data['#TE'].str.find('Gypsy') != -1) & (data.Order == 'LTR') & (data.Superfamily == 'Gypsy')]\n",
    "    ltr_copia = data[(data['#TE'].str.find('LTR') != -1) & (data['#TE'].str.find('Copia') != -1) & (data.Order == 'LTR') & (data.Superfamily == 'Copia')]\n",
    "    ltr_gypsy_stats['EDTA/TEsorter_Classification_Count'] = len(ltr_gypsy.index)\n",
    "    ltr_copia_stats['EDTA/TEsorter_Classification_Count'] = len(ltr_copia.index)\n",
    "\n",
    "    # Remove incorrect coding domains classifications\n",
    "    def has_correct_domains(clade, domains):\n",
    "        split_domains = domains.split(' ')\n",
    "        for domain in split_domains:\n",
    "            if clade not in domain:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    ltr_gypsy = ltr_gypsy[ltr_gypsy.apply(lambda row: has_correct_domains(row['Clade'], row['Domains']), axis=1)]\n",
    "    ltr_copia = ltr_copia[ltr_copia.apply(lambda row: has_correct_domains(row['Clade'], row['Domains']), axis=1)]\n",
    "    ltr_gypsy_stats['Correct_Domains_Count'] = len(ltr_gypsy.index)\n",
    "    ltr_copia_stats['Correct_Domains_Count'] = len(ltr_copia.index)\n",
    "\n",
    "    # Remove clade from domains\n",
    "    ltr_gypsy.Domains = ltr_gypsy.Domains.str.replace(\"\\|[^\\s]*\", \"\", regex=True)\n",
    "    ltr_copia.Domains = ltr_copia.Domains.str.replace(\"\\|[^\\s]*\", \"\", regex=True)\n",
    "\n",
    "    # Removing extra coding domains\n",
    "    # - aRH to RH\n",
    "    ltr_gypsy.Domains = ltr_gypsy.Domains.apply(lambda domains: domains.replace('aRH', 'RH'))\n",
    "    ltr_copia.Domains = ltr_copia.Domains.apply(lambda domains: domains.replace('aRH', 'RH'))\n",
    "\n",
    "    # - Remove CHD & CHDCR\n",
    "    ltr_gypsy.Domains = ltr_gypsy.Domains.str.replace(\"CHDCR\", \"\", regex=True).str.strip()\n",
    "    ltr_gypsy.Domains = ltr_gypsy.Domains.str.replace(\"CHD\", \"\", regex=True).str.strip()\n",
    "    ltr_gypsy.Domains = ltr_gypsy.Domains.str.replace(\"\\s\\s\", \" \", regex=True).str.strip()\n",
    "\n",
    "    ltr_copia.Domains = ltr_copia.Domains.str.replace(\"CHDCR\", \"\", regex=True).str.strip()\n",
    "    ltr_copia.Domains = ltr_copia.Domains.str.replace(\"CHD\", \"\", regex=True).str.strip()\n",
    "    ltr_copia.Domains = ltr_copia.Domains.str.replace(\"\\s\\s\", \" \", regex=True).str.strip()\n",
    "\n",
    "    # Remove elements with singular gene (except GAG)\n",
    "    def has_multiple_domain(domains):\n",
    "        split_domains = domains.split(' ')\n",
    "        return len(split_domains) > 1 or (len(split_domains) == 1 and split_domains[0] == \"GAG\")\n",
    "\n",
    "    ltr_gypsy = ltr_gypsy[ltr_gypsy.apply(lambda row: has_multiple_domain(row['Domains']), axis=1)]\n",
    "    ltr_copia = ltr_copia[ltr_copia.apply(lambda row: has_multiple_domain(row['Domains']), axis=1)]\n",
    "\n",
    "    # Remove elements with incorrect domain order\n",
    "    gypsy_domain_order = [\"GAG\", \"PROT\", \"RT\", \"RH\", \"INT\"]\n",
    "    copia_domain_order = [\"GAG\", \"PROT\", \"INT\", \"RT\", \"RH\"]\n",
    "\n",
    "    def has_correct_domain_order(domain_order, domains):\n",
    "        split_domains = domains.split(' ')\n",
    "        place_in_sequence = 0\n",
    "        for domain in split_domains:\n",
    "            index_of_value = domain_order.index(domain)\n",
    "            if index_of_value < place_in_sequence:\n",
    "                return False\n",
    "            else:\n",
    "                place_in_sequence = index_of_value\n",
    "\n",
    "        return True\n",
    "\n",
    "    ltr_gypsy = ltr_gypsy[ltr_gypsy.apply(lambda row: has_correct_domain_order(gypsy_domain_order, row['Domains']), axis=1)]\n",
    "    ltr_copia = ltr_copia[ltr_copia.apply(lambda row: has_correct_domain_order(copia_domain_order, row['Domains']), axis=1)]\n",
    "\n",
    "    # Remove elements with multiple deletion events\n",
    "    def does_not_have_multiple_deletion_events(domain_order, domains):\n",
    "        split_domains = domains.split(' ')\n",
    "        place_in_sequence = -1\n",
    "        has_deletion_event = False\n",
    "\n",
    "        for domain in split_domains:\n",
    "            index_of_value = domain_order.index(domain)\n",
    "\n",
    "            if (index_of_value - place_in_sequence) > 1:\n",
    "                if has_deletion_event:\n",
    "                    return False\n",
    "                has_deletion_event = True\n",
    "\n",
    "            place_in_sequence = index_of_value\n",
    "\n",
    "        if has_deletion_event & ((domain_order.index(split_domains[-1])) != (len(domain_order) - 1)):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    ltr_gypsy = ltr_gypsy[ltr_gypsy.apply(lambda row: does_not_have_multiple_deletion_events(gypsy_domain_order, row['Domains']), axis=1)]\n",
    "    ltr_copia = ltr_copia[ltr_copia.apply(lambda row: does_not_have_multiple_deletion_events(copia_domain_order, row['Domains']), axis=1)]\n",
    "    ltr_gypsy_stats['Processed_Domains_Count'] = len(ltr_gypsy.index)\n",
    "    ltr_copia_stats['Processed_Domains_Count'] = len(ltr_copia.index)\n",
    "\n",
    "\n",
    "    return [ltr_gypsy, ltr_copia, pd.DataFrame.from_dict(ltr_gypsy_stats, orient='index').T, pd.DataFrame.from_dict(ltr_copia_stats, orient='index').T]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T21:17:33.379077500Z",
     "start_time": "2023-12-17T21:17:25.508112800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aalpi_genome.fasta.mod.EDTA.intact.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "Asuec_genome.fasta.mod.EDTA.intact.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "Bnigr_genome.fasta.mod.EDTA.intact.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "daArcMinu1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "daBalNigr1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "daLinVulg1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "daLycEuro1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "daMisOron1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "daPulDyse1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "daScuGale1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "daSheArve1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "daSolDulc1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "dcPolAvic1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "ddEupPepu3.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "ddMerAnnu1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "dhAlnGlut1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "dhQueRobu3.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drAilAlti1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drChaAngu1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drFilUlma1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drGeuUrba1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drHedHeli1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drMalDome5.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drMalSylv7.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drMedArab1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drParJuda1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drPotAnse1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drTriFrag1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n",
      "drUrtUren1.1.fa.mod.EDTA.TEanno.fa.rexdb-plant.cls.tsv.gz\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = 'all-data'\n",
    "stat_out = [pd.DataFrame(), pd.DataFrame()]\n",
    "data_out = [pd.DataFrame(), pd.DataFrame()]\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    print(filename)\n",
    "    print(\"\\n\")\n",
    "    processed_data = process_data(filename, pd.read_csv(directory + \"/\" + filename, sep='\\t').head(10000))\n",
    "    data_out = [pd.concat([data_out[0], processed_data[0]], ignore_index=True), pd.concat([data_out[1], processed_data[1]], ignore_index=True)]\n",
    "    stat_out = [pd.concat([stat_out[0], processed_data[2]], ignore_index=True), pd.concat([stat_out[1], processed_data[3]], ignore_index=True)]\n",
    "\n",
    "if not os.path.exists('out'):\n",
    "    os.makedirs('out')\n",
    "\n",
    "data_out[0].to_csv('out/ltr_gypsy_processed.tsv', sep=\"\\t\")\n",
    "data_out[1].to_csv('out/ltr_copia_processed.tsv', sep=\"\\t\")\n",
    "stat_out[0].to_csv('out/ltr_gypsy_stats.tsv', sep=\"\\t\")\n",
    "stat_out[1].to_csv('out/ltr_copia_stats.tsv', sep=\"\\t\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
